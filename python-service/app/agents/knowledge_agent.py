from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
import pydantic_ai
from openai import AsyncOpenAI
from pydantic_ai import Agent, RunContext
import asyncio

# Importamos el estado global y funciones de Zep
from ..state import GlobalState
from ..zep import get_zep_memory_context, search_zep_facts, search_zep_sessions, search_zep_nodes
from ..db import supabase_client

# --- Cliente Pydantic AI ---
# Usamos un cliente de OpenAI independiente para este agente
# para mantener los contextos de herramientas separados.
client = AsyncOpenAI()

# --- Funciones Auxiliares para B√∫squeda Sem√°ntica ---

async def generate_embedding(text: str) -> List[float]:
    """
    Genera un embedding para el texto usando OpenAI.
    """
    try:
        response = await client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"‚ùå Error generando embedding: {e}")
        return []

async def search_knowledge_semantic(query: str, organization_id: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Busca informaci√≥n (servicios y archivos) usando b√∫squeda sem√°ntica en la knowledge_base.
    
    Args:
        query: Consulta de b√∫squeda del usuario
        organization_id: ID de la organizaci√≥n 
        limit: N√∫mero m√°ximo de resultados
    
    Returns:
        Lista de contenido encontrado (servicios y archivos) con sus metadatos
    """
    try:
        import time
        start_time = time.time()
        print(f"[{start_time:.2f}] --- Iniciando b√∫squeda sem√°ntica para: '{query}'")

        # Generar embedding de la consulta (esta funci√≥n s√≠ es async y debe ser esperada)
        query_embedding = await generate_embedding(query)
        
        if not query_embedding:
            return []
        
        embedding_time = time.time()
        print(f"[{embedding_time:.2f}] --- Embedding generado en {embedding_time - start_time:.2f}s")

        # Buscar en knowledge_base usando la sintaxis correcta para ejecutar
        # una llamada s√≠ncrona (rpc) desde un contexto as√≠ncrono.
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None, 
            lambda: supabase_client.rpc(
                'match_documents_by_org',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': 0.2,
                    'match_count': limit,
                    'org_id': organization_id
                }
            ).execute()
        )

        rpc_time = time.time()
        print(f"[{rpc_time:.2f}] --- RPC a Supabase completado en {rpc_time - embedding_time:.2f}s")
        print(f"üîç DEBUG: RPC retorn√≥ {len(result.data) if result.data else 0} resultados")

        results_found = []
        seen_items = set()
        
        if result.data:
            for item in result.data:
                metadata = item.get('metadata', {})
                source_type = metadata.get('source_type')
                
                if source_type == 'service':
                    service_id = metadata.get('service_id')
                    # Evitar duplicados del mismo servicio
                    if service_id and service_id not in seen_items:
                        seen_items.add(service_id)
                        results_found.append({
                            'service_id': service_id,
                            'content': item.get('content', ''),
                            'similarity': item.get('similarity', 0),
                            'metadata': metadata
                        })
                        
                elif source_type == 'file':
                    # Para archivos, usamos una combinaci√≥n √∫nica de file_name + category
                    file_name = metadata.get('file_name', '')
                    category = metadata.get('category', '')
                    unique_key = f"{file_name}_{category}"
                    
                    if unique_key not in seen_items:
                        seen_items.add(unique_key)
                        results_found.append({
                            'content': item.get('content', ''),
                            'similarity': item.get('similarity', 0),
                            'metadata': metadata
                        })
        
        end_time = time.time()
        print(f"[{end_time:.2f}] --- B√∫squeda sem√°ntica completada en {end_time - start_time:.2f}s")
        return results_found
        
    except Exception as e:
        print(f"‚ùå Error en b√∫squeda sem√°ntica: {e}")
        return []

async def get_service_by_id(service_id: str) -> Optional[Dict[str, Any]]:
    """
    Obtiene informaci√≥n completa de un servicio por su ID.
    """
    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: supabase_client.table('services').select('*').eq('id', service_id).execute()
        )
        
        if result.data:
            return result.data[0]
        return None
        
    except Exception as e:
        print(f"‚ùå Error obteniendo servicio {service_id}: {e}")
        return None



# --- Modelos de Respuesta ---

class KnowledgeSearchResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de conocimiento."""
    # Para servicios
    service_id: Optional[str] = Field(default=None, description="El ID √∫nico del servicio encontrado.")
    service_name: Optional[str] = Field(default=None, description="Nombre del servicio encontrado.")
    requires_assessment: Optional[bool] = Field(default=None, description="Si el servicio requiere valoraci√≥n previa.")
    
    # Para informaci√≥n general (archivos)
    information_found: Optional[str] = Field(default=None, description="Informaci√≥n espec√≠fica encontrada (ubicaci√≥n, horarios, contacto, etc.)")
    source_type: Optional[str] = Field(default=None, description="Tipo de fuente: 'service' o 'file'")
    category: Optional[str] = Field(default=None, description="Categor√≠a del archivo si es de tipo 'file'")
    
    # Para casos especiales
    clarification_message: Optional[str] = Field(default=None, description="Mensaje para pedir aclaraci√≥n al usuario si la b√∫squeda es ambigua o no arroja resultados.")



class UserFactsResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de hechos del usuario."""
    facts_found: List[str] = Field(default_factory=list, description="Lista de hechos encontrados sobre el usuario.")
    summary: str = Field(description="Resumen de los hechos encontrados.")

class UserSessionsResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de conversaciones pasadas."""
    conversations_found: List[str] = Field(default_factory=list, description="Lista de fragmentos de conversaciones relevantes.")
    summary: str = Field(description="Resumen de las conversaciones encontradas.")

class UserInsightsResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de insights del usuario."""
    insights_found: List[str] = Field(default_factory=list, description="Lista de insights/res√∫menes encontrados.")
    summary: str = Field(description="Resumen de los insights encontrados.")

# --- Definici√≥n del Agente de Conocimiento ---
knowledge_agent = Agent[GlobalState](
    'openai:gpt-4o', 
    deps_type=GlobalState,
    system_prompt="""
    Eres un asistente amigable y experto en servicios y productos de la empresa.
    Habla de manera natural y conversacional, como si fueras un asesor personal que conoce bien los servicios.
    Tu trabajo es ayudar al usuario a encontrar y seleccionar el servicio que desea, utilizando tanto la informaci√≥n actual como el historial del usuario.

    HERRAMIENTAS DISPONIBLES:
    
    1. B√öSQUEDA DE INFORMACI√ìN:
       - 'knowledge_search': Busca servicios Y informaci√≥n general usando IA sem√°ntica
       - NO necesitas palabras exactas: busca por significado e intenci√≥n
       - Encuentra autom√°ticamente: ubicaci√≥n, horarios, contacto, FAQ, pol√≠ticas, promociones, etc.
    
    2. B√öSQUEDA DE HISTORIAL DEL USUARIO (√∫salas cuando sea relevante):
       - 'search_user_facts': Busca hechos espec√≠ficos (servicios previos, preferencias, alergias, etc.)
       - 'search_user_conversations': Busca conversaciones pasadas (recomendaciones, quejas, etc.)
       - 'search_user_insights': Busca patrones de comportamiento del usuario
    
    FLUJO DE TRABAJO:
    
    1. EVAL√öA LA CONSULTA: ¬øEl usuario menciona algo del pasado, historial, o "antes"?
       - SI: Usa herramientas de b√∫squeda de historial primero
       - NO: Procede con b√∫squeda de servicios
    
    2. B√öSQUEDA DE INFORMACI√ìN:
       - Usa 'knowledge_search' para encontrar servicios O informaci√≥n general
       - Para servicios: incluye nombre, descripci√≥n y detalles relevantes
       - Para info general: incluye contenido directo (ubicaci√≥n, horarios, etc.)
       - Si necesitas aclaraci√≥n, devuelve el mensaje y termina
    
    3. PERSONALIZACI√ìN CON HISTORIAL:
       - Si es relevante, busca informaci√≥n previa del usuario
       - Combina la informaci√≥n actual con el historial para dar respuestas personalizadas
    
    EJEMPLOS DE USO:
    
    INFORMACI√ìN GENERAL (b√∫squeda sem√°ntica inteligente):
    - "¬øD√≥nde quedan?" / "direcci√≥n" / "ubicaci√≥n" ‚Üí Encuentra info de ubicaci√≥n
    - "¬øA qu√© hora abren?" / "horarios" / "cu√°ndo atienden?" ‚Üí Encuentra horarios
    - "¬øTel√©fono?" / "contacto" / "WhatsApp" ‚Üí Encuentra info de contacto
    - "¬øOfertas?" / "descuentos" / "promociones" ‚Üí Encuentra promociones
    - "¬øDudas frecuentes?" / "FAQ" / "preguntas" ‚Üí Encuentra FAQ
    - "¬øPol√≠ticas?" / "reglas" / "t√©rminos" ‚Üí Encuentra pol√≠ticas
    
    SERVICIOS:
    - "Quiero un servicio que me ayude con [problema]" ‚Üí Busca servicios
    - "¬øCu√°nto cuesta [nombre del servicio]?" ‚Üí Busca servicio espec√≠fico
    
    HISTORIAL DEL USUARIO:
    - "¬øQu√© servicios me recomendaste antes?" ‚Üí search_user_conversations
    - "¬øCu√°les fueron mis servicios favoritos?" ‚Üí search_user_facts
    - "¬øTuve alg√∫n problema?" ‚Üí search_user_facts
    - "¬øHe venido antes por este tipo de problema?"
    
    üö® REGLAS CR√çTICAS:
    - NUNCA INVENTES informaci√≥n que no encuentres en las herramientas
    - NO menciones servicios espec√≠ficos a menos que la herramienta los devuelva expl√≠citamente
    - Si no encuentras informaci√≥n espec√≠fica, pide clarificaci√≥n o ofrece escalaci√≥n
    - SOLO usa datos reales retornados por las herramientas de b√∫squeda
    - NO hagas suposiciones sobre qu√© servicios podr√≠an existir
    
    üìù FORMATO DE RESPUESTAS:
    - Habla de manera natural y conversacional, evita listas t√©cnicas
    - Usa un tono amigable como "Te cuento que...", "Mira, tenemos...", "Perfecto, aqu√≠ est√°..."
    - Organiza la informaci√≥n de manera fluida, no como puntos de lista
    - Cuando presentes m√∫ltiples servicios, hazlo como si estuvieras recomendando personalmente
    - USA EMOJIS para hacer la conversaci√≥n m√°s natural y amigable üòä
    - NO uses asteriscos (**texto**) ni formato markdown para t√≠tulos
    - Escribe los nombres de servicios de forma natural, como en una conversaci√≥n normal
    
    üö® OBLIGATORIO: SIEMPRE debes usar las herramientas disponibles para buscar informaci√≥n. NUNCA respondas directamente sin usar herramientas.
    
    - Para CUALQUIER consulta sobre servicios, ubicaci√≥n, horarios, precios ‚Üí USA 'knowledge_search'
    - Para preguntas sobre historial del usuario ‚Üí USA las herramientas de b√∫squeda de usuario
    - NO tengas conversaciones directas, SIEMPRE delega a las herramientas correspondientes
    """
)

# --- Herramientas del Agente ---

@knowledge_agent.tool
async def knowledge_search(ctx: RunContext[GlobalState], query: str) -> KnowledgeSearchResult:
    """
    Busca cualquier informaci√≥n (servicios, ubicaci√≥n, horarios, contacto, etc.) usando b√∫squeda sem√°ntica.
    Si encuentra informaci√≥n relevante, la devuelve. Si no, ofrece escalaci√≥n a asesor.
    """
    state = ctx.deps
    organization_id = state.get("organization_id")
    
    if not organization_id:
        return KnowledgeSearchResult(clarification_message="Error: No se pudo identificar la organizaci√≥n.")
    
    print(f"üîç Buscando informaci√≥n para: '{query}' en organizaci√≥n {organization_id}")
    
    try:
        # Buscar con l√≠mite m√°s alto para capturar todos los chunks de un servicio
        matching_results = await search_knowledge_semantic(query, organization_id, limit=10)
        
        if not matching_results:
            return KnowledgeSearchResult(
                clarification_message=f"No encontr√© informaci√≥n espec√≠fica sobre '{query}'. ¬øTe gustar√≠a hablar con un asesor que pueda ayudarte mejor?"
            )
        
        # Analizar si es una b√∫squeda amplia de servicios
        query_lower = query.lower()
        is_broad_service_query = any(keyword in query_lower for keyword in [
            'servicios', 'qu√© tienen', 'qu√© ofrecen', 'cat√°logo', 'tratamientos', 
            'ofertas', 'que hacen', 'especialidades', 'procedimientos'
        ])
        
        # Analizar el tipo de resultado y consolidar informaci√≥n si es necesario
        best_result = matching_results[0]
        metadata = best_result.get('metadata', {})
        source_type = metadata.get('source_type')
        similarity = best_result.get('similarity', 0)
        
        print(f"‚úÖ Informaci√≥n encontrada: source_type={source_type}, similarity={similarity:.2f}")
        print(f"üîç B√∫squeda amplia de servicios detectada: {is_broad_service_query}")
        
        # üéØ UMBRAL DE SIMILARITY: Si la similarity es muy baja, no es informaci√≥n √∫til
        if similarity < 0.1:  # Ajustar este valor seg√∫n necesidades
            return KnowledgeSearchResult(
                clarification_message=f"No encontr√© informaci√≥n espec√≠fica sobre '{query}'. ¬øPodr√≠as ser m√°s espec√≠fico sobre qu√© tipo de informaci√≥n necesitas? Por ejemplo: servicios, precios, ubicaci√≥n, horarios, etc. Si prefieres, tambi√©n puedo ayudarte a contactar con un asesor."
            )
        
        # üìã MANEJO ESPECIAL PARA B√öSQUEDAS AMPLIAS DE SERVICIOS
        if is_broad_service_query and source_type == 'service':
            # Obtener servicios √∫nicos de los resultados
            unique_services = {}
            for result in matching_results:
                result_metadata = result.get('metadata', {})
                if result_metadata.get('source_type') == 'service':
                    service_id = result_metadata.get('service_id')
                    similarity_score = result.get('similarity', 0)
                    
                    # Solo incluir si tiene buena similarity
                    if similarity_score >= 0.1 and service_id:
                        if service_id not in unique_services or similarity_score > unique_services[service_id]['similarity']:
                            unique_services[service_id] = {
                                'content': result.get('content', ''),
                                'similarity': similarity_score,
                                'metadata': result_metadata
                            }
            
            # Si tenemos m√∫ltiples servicios, mostrarlos todos
            if len(unique_services) > 1:
                services_info = []
                for service_id, service_data in unique_services.items():
                    content = service_data['content']
                    # Extraer nombre del servicio de los metadatos o contenido
                    service_name = service_data['metadata'].get('service_name', 'Servicio')
                    services_info.append(f"üåü {service_name}\n{content}")
                
                consolidated_services = "\n\n".join(services_info)
                
                # Agregar sugerencia inteligente para m√°s detalles
                suggestion = "\n\nüí° Si quieres informaci√≥n m√°s detallada sobre alg√∫n servicio espec√≠fico (como precios, contraindicaciones, o cuidados), solo menciona el nombre del servicio que te interesa üòä"
                
                return KnowledgeSearchResult(
                    information_found=consolidated_services + suggestion,
                    source_type='multiple_services',
                    category='servicios'
                )
            
            # Si solo hay un servicio pero era una b√∫squeda amplia, sugerir m√°s opciones
            elif len(unique_services) == 1:
                # Procesar el √∫nico servicio encontrado pero agregar sugerencia
                service_data = list(unique_services.values())[0]
                content = service_data['content']
                suggestion = "\n\nüí° Este es uno de nuestros servicios. Si buscas algo espec√≠fico o quieres conocer otros servicios disponibles, puedes preguntarme por el tipo de tratamiento que te interesa üòä"
                
                return KnowledgeSearchResult(
                    information_found=content + suggestion,
                    source_type='service',
                    service_id=list(unique_services.keys())[0]
                )
        
        if source_type == 'file':
            # Es informaci√≥n general (ubicaci√≥n, horarios, etc.) - usar solo el mejor resultado
            content = best_result.get('content', '')
            print(f"üìÑ Content preview: {content[:100]}...")
            return KnowledgeSearchResult(
                information_found=content,
                source_type='file',
                category=metadata.get('category', 'general')
            )
        
        elif source_type == 'service':
            # Es informaci√≥n de un servicio - CONSOLIDAR TODOS LOS CHUNKS DEL MISMO SERVICIO
            service_id = metadata.get('service_id')
            service_name = metadata.get('service_name', 'Servicio')
            
            print(f"üîç DEBUG: Buscando chunks para service_id={service_id}")
            print(f"üîç DEBUG: Total de resultados recibidos: {len(matching_results)}")
            
            # DEBUG: Mostrar todos los resultados para diagnosticar
            for i, result in enumerate(matching_results):
                result_metadata = result.get('metadata', {})
                result_service_id = result_metadata.get('service_id')
                result_source_type = result_metadata.get('source_type')
                result_similarity = result.get('similarity', 0)
                print(f"üîç DEBUG: Resultado {i+1} - service_id={result_service_id}, source_type={result_source_type}, similarity={result_similarity:.2f}")
            
            # Filtrar todos los resultados del mismo servicio (con similarity m√≠nima)
            service_chunks = []
            for r in matching_results:
                r_metadata = r.get('metadata', {})
                r_service_id = r_metadata.get('service_id')
                r_source_type = r_metadata.get('source_type')
                r_similarity = r.get('similarity', 0)
                
                # Incluir si es del mismo servicio Y tiene buena similarity
                if (r_source_type == 'service' and 
                    r_service_id == service_id and 
                    r_similarity >= 0.1):
                    service_chunks.append(r)
            
            print(f"üìã DEBUG: Chunks del mismo servicio encontrados: {len(service_chunks)}")
            
            # Consolidar toda la informaci√≥n del servicio
            consolidated_content = []
            for chunk in service_chunks:
                chunk_content = chunk.get('content', '')
                chunk_similarity = chunk.get('similarity', 0)
                print(f"üìÑ Chunk encontrado (similitud: {chunk_similarity:.2f}): {chunk_content[:80]}...")
                consolidated_content.append(chunk_content)
            
            # Si no encontramos chunks adicionales, buscar otros chunks de servicio con similarity alta
            if len(service_chunks) == 1:
                print("üîç Solo se encontr√≥ 1 chunk, buscando otros chunks de servicios relacionados...")
                for r in matching_results:
                    r_metadata = r.get('metadata', {})
                    r_source_type = r_metadata.get('source_type')
                    r_similarity = r.get('similarity', 0)
                    
                    # Incluir otros chunks de servicios con alta similarity que no hayamos incluido ya
                    if (r_source_type == 'service' and 
                        r_similarity >= 0.3 and  # Similarity m√°s alta para otros servicios
                        r not in service_chunks):
                        
                        chunk_content = r.get('content', '')
                        print(f"üìÑ Chunk adicional relacionado (similitud: {r_similarity:.2f}): {chunk_content[:80]}...")
                        consolidated_content.append(chunk_content)
            
            # Unir todo el contenido con formato m√°s conversacional
            full_service_info = "\n\n".join(consolidated_content)
            print(f"üìã Informaci√≥n consolidada del servicio ({len(consolidated_content)} chunks)")
            
            # Formatear de manera m√°s conversacional
            conversational_info = f"Te cuento sobre {service_name} ‚ú®\n\n{full_service_info}"
            
            return KnowledgeSearchResult(
                information_found=conversational_info,
                source_type='service',
                service_id=service_id
            )
        
        else:
            # Tipo de fuente desconocido, devolver contenido como informaci√≥n general
            content = best_result.get('content', '')
            return KnowledgeSearchResult(
                information_found=content,
                source_type='unknown'
            )
            
    except Exception as e:
        print(f"‚ùå Error en knowledge_search: {e}")
        return KnowledgeSearchResult(
            clarification_message="Hubo un problema al buscar informaci√≥n. ¬øTe gustar√≠a hablar con un asesor?"
        )


@knowledge_agent.tool
async def search_user_facts(ctx: RunContext[GlobalState], query: str) -> UserFactsResult:
    """
    Busca hechos espec√≠ficos sobre el usuario actual en su historial.
    √ötil para encontrar preferencias, servicios previos, problemas reportados, etc.
    
    Ejemplos de uso:
    - "servicios previos" 
    - "problemas reportados"
    - "preferencias de horarios"
    - "alergias o restricciones"
    """
    state = ctx.deps
    print(f"üîç Buscando hechos del usuario para: '{query}'")
    
    if not state.get("chat_identity_id"):
        return UserFactsResult(
            facts_found=[],
            summary="No hay informaci√≥n del usuario disponible en esta sesi√≥n."
        )
    
    # Construir user_id como en main.py
    if state.get("contact_id"):
        user_id = f"contact_{state['contact_id']}"
    else:
        user_id = f"chat_{state['chat_identity_id']}"
    
    try:
        facts = await search_zep_facts(user_id=user_id, query=query, limit=5)
        
        if facts:
            summary = f"Encontr√© {len(facts)} hechos relevantes sobre '{query}'"
            print(f"‚úÖ {summary}")
            return UserFactsResult(facts_found=facts, summary=summary)
        else:
            summary = f"No encontr√© informaci√≥n espec√≠fica sobre '{query}' en el historial del usuario."
            print(f"‚ùå {summary}")
            return UserFactsResult(facts_found=[], summary=summary)
            
    except Exception as e:
        print(f"‚ùå Error buscando hechos: {e}")
        return UserFactsResult(
            facts_found=[],
            summary="Error al buscar en el historial del usuario."
        )

@knowledge_agent.tool
async def search_user_conversations(ctx: RunContext[GlobalState], query: str) -> UserSessionsResult:
    """
    Busca conversaciones pasadas del usuario que contengan informaci√≥n espec√≠fica.
    √ötil para recordar discusiones previas, recomendaciones hechas, etc.
    
    Ejemplos de uso:
    - "citas canceladas"
    - "servicios recomendados"
    - "quejas o problemas"
    - "horarios preferidos"
    """
    state = ctx.deps
    print(f"üîç Buscando conversaciones del usuario para: '{query}'")
    
    if not state.get("chat_identity_id"):
        return UserSessionsResult(
            conversations_found=[],
            summary="No hay historial de conversaciones disponible en esta sesi√≥n."
        )
    
    # Construir user_id como en main.py
    if state.get("contact_id"):
        user_id = f"contact_{state['contact_id']}"
    else:
        user_id = f"chat_{state['chat_identity_id']}"
    
    try:
        conversations = await search_zep_sessions(user_id=user_id, query=query, limit=3)
        
        if conversations:
            summary = f"Encontr√© {len(conversations)} conversaciones relevantes sobre '{query}'"
            print(f"‚úÖ {summary}")
            return UserSessionsResult(conversations_found=conversations, summary=summary)
        else:
            summary = f"No encontr√© conversaciones previas sobre '{query}'."
            print(f"‚ùå {summary}")
            return UserSessionsResult(conversations_found=[], summary=summary)
            
    except Exception as e:
        print(f"‚ùå Error buscando conversaciones: {e}")
        return UserSessionsResult(
            conversations_found=[],
            summary="Error al buscar en el historial de conversaciones."
        )

@knowledge_agent.tool
async def search_user_insights(ctx: RunContext[GlobalState], query: str) -> UserInsightsResult:
    """
    Busca insights y res√∫menes sobre el comportamiento del usuario.
    √ötil para entender patrones, preferencias generales, perfil del cliente.
    
    Ejemplos de uso:
    - "perfil del cliente"
    - "comportamiento de compra"
    - "satisfacci√≥n con servicios"
    - "tendencias de uso"
    """
    state = ctx.deps
    print(f"üîç Buscando insights del usuario para: '{query}'")
    
    if not state.get("chat_identity_id"):
        return UserInsightsResult(
            insights_found=[],
            summary="No hay informaci√≥n de insights disponible en esta sesi√≥n."
        )
    
    # Construir user_id como en main.py
    if state.get("contact_id"):
        user_id = f"contact_{state['contact_id']}"
    else:
        user_id = f"chat_{state['chat_identity_id']}"
    
    try:
        insights = await search_zep_nodes(user_id=user_id, query=query, limit=3)
        
        if insights:
            summary = f"Encontr√© {len(insights)} insights relevantes sobre '{query}'"
            print(f"‚úÖ {summary}")
            return UserInsightsResult(insights_found=insights, summary=summary)
        else:
            summary = f"No encontr√© insights espec√≠ficos sobre '{query}'."
            print(f"‚ùå {summary}")
            return UserInsightsResult(insights_found=[], summary=summary)
            
    except Exception as e:
        print(f"‚ùå Error buscando insights: {e}")
        return UserInsightsResult(
            insights_found=[],
            summary="Error al buscar insights del usuario."
        )

# --- Funci√≥n de Entrada (Entrypoint) para el Grafo ---
from langgraph.types import Command
from langgraph.graph import END

async def run_knowledge_agent(state: GlobalState) -> Command:
    """
    Punto de entrada para ejecutar el agente de conocimiento.
    Ahora incluye contexto de memoria de Zep y usa Command pattern para evitar loops.
    """
    print("--- Ejecutando Knowledge Agent ---")
    
    user_query = state['messages'][-1].content
    
    # Detectar si venimos del AppointmentAgent
    is_service_resolution = "Necesito encontrar el servicio espec√≠fico que quiere agendar" in user_query
    print(f"üîç KNOWLEDGE AGENT: Modo resoluci√≥n de servicio: {is_service_resolution}")
    
    # Obtener contexto de memoria de Zep si hay chat_identity_id
    zep_context = ""
    if state.get("chat_identity_id"):
        thread_id = state['chat_identity_id']
        try:
            zep_context = await get_zep_memory_context(thread_id, min_rating=0.0)
        except Exception as e:
            print(f"‚ùå Error obteniendo contexto de Zep thread {thread_id}: {e}")
            zep_context = ""
    
    # Construir query enriquecido con contexto de Zep
    if is_service_resolution and zep_context:
        # Cuando venimos del appointment_agent, extraer el servicio del contexto
        enhanced_query = f"Buscar servicio espec√≠fico mencionado en: {zep_context}"
        print(f"üîç KNOWLEDGE AGENT: Query de resoluci√≥n: {enhanced_query}")
    else:
        enhanced_query = user_query
        if zep_context:
            enhanced_query = f"Contexto del usuario: {zep_context}\n\nConsulta actual: {user_query}"
    
    result = await knowledge_agent.run(enhanced_query, deps=state)
    
    # El resultado del agente puede ser de varios tipos, lo procesamos
    tool_output = result.output
    print(f"üîç DEBUG: tool_output type: {type(tool_output)}")
    print(f"üîç DEBUG: tool_output value: {tool_output}")

    # Obtener mensajes actuales para conservar el historial
    current_messages = state.get("messages", [])

    if isinstance(tool_output, KnowledgeSearchResult):
        print(f"üîç DEBUG KnowledgeSearchResult:")
        print(f"üîç clarification_message: {bool(tool_output.clarification_message)}")
        print(f"üîç service_id: {tool_output.service_id}")
        print(f"üîç information_found: {bool(tool_output.information_found)}")
        
        if tool_output.clarification_message:
            # Si la herramienta devuelve un mensaje de clarificaci√≥n (incluso si fall√≥),
            # lo tratamos como la respuesta final de este turno para romper el bucle.
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=tool_output.clarification_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        elif tool_output.service_id:
            # Si se encontr√≥ un servicio, incluir informaci√≥n de valoraci√≥n si est√° disponible
            # y enrutar al AppointmentAgent para posible agendamiento
            print(f"üìã KNOWLEDGE AGENT: PATH service_id - Actualizando estado con service_id: {tool_output.service_id}")
            
            result_data = {
                "service_id": tool_output.service_id,
                "messages": current_messages
            }
            if tool_output.requires_assessment is not None:
                result_data["requires_assessment"] = tool_output.requires_assessment
            if tool_output.service_name:
                result_data["service_name"] = tool_output.service_name
                print(f"üìã KNOWLEDGE AGENT: PATH service_id - Actualizando estado con service_name: {tool_output.service_name}")
            
            print(f"üìã KNOWLEDGE AGENT: PATH service_id - result_data completo: {result_data}")
            
            # Si estamos en modo resoluci√≥n, regresar directamente al AppointmentAgent
            if is_service_resolution:
                print(f"üìã KNOWLEDGE AGENT: Modo resoluci√≥n - regresando a AppointmentAgent con service_id")
                return Command(
                    update=result_data,
                    goto="AppointmentAgent"
                )
            else:
                # Retornar al supervisor para que decida si ir a AppointmentAgent
                return Command(
                    update=result_data,
                    goto="Supervisor"
                )
        elif tool_output.information_found:
            # Si se encontr√≥ informaci√≥n general (archivos o servicios), devolverla directamente y terminar
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=tool_output.information_found, name="KnowledgeAgent")
            
            # IMPORTANTE: Si es informaci√≥n de un servicio, tambi√©n actualizar el service_id en el estado
            update_data = {"messages": current_messages + [ai_message]}
            if tool_output.service_id:
                update_data["service_id"] = tool_output.service_id
                print(f"üìã KNOWLEDGE AGENT: Actualizando estado con service_id: {tool_output.service_id}")
                print(f"üìã KNOWLEDGE AGENT: update_data completo: {update_data}")
            if tool_output.service_name:
                update_data["service_name"] = tool_output.service_name
                print(f"üìã KNOWLEDGE AGENT: Actualizando estado con service_name: {tool_output.service_name}")
            if tool_output.requires_assessment is not None:
                update_data["requires_assessment"] = tool_output.requires_assessment
            
            return Command(
                update=update_data,
                goto="__end__"
            )
    
    elif isinstance(tool_output, UserFactsResult):
        if tool_output.facts_found:
            facts_text = "\n".join([f"- {fact}" for fact in tool_output.facts_found])
            response_message = f"{tool_output.summary}\n\n{facts_text}"
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=response_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        else:
            # Si no se encuentran hechos, regresar al supervisor para que decida
            return Command(goto="Supervisor")
    
    elif isinstance(tool_output, UserSessionsResult):
        if tool_output.conversations_found:
            conversations_text = "\n".join([f"- {conv}" for conv in tool_output.conversations_found])
            response_message = f"{tool_output.summary}\n\n{conversations_text}"
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=response_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        else:
            return Command(goto="Supervisor")

    elif isinstance(tool_output, UserInsightsResult):
        if tool_output.insights_found:
            insights_text = "\n".join([f"- {insight}" for insight in tool_output.insights_found])
            response_message = f"{tool_output.summary}\n\n{insights_text}"
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=response_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        else:
            return Command(goto="Supervisor")

    # Si el resultado es un string directo (respuesta del LLM)
    elif isinstance(tool_output, str):
        from langchain_core.messages import AIMessage
        ai_message = AIMessage(content=tool_output, name="KnowledgeAgent")
        return Command(
            update={"messages": current_messages + [ai_message]},
            goto="__end__"
        )

    # Caso por defecto o si la salida no es lo que esperamos (ej. b√∫squeda vac√≠a)
    print("‚ö†Ô∏è El Knowledge Agent no produjo una salida clara. Terminando para evitar bucle.")
    from langchain_core.messages import AIMessage
    ai_message = AIMessage(content="No estoy seguro de c√≥mo proceder. ¬øPuedes reformular tu pregunta? Si lo prefieres, puedo contactar a un asesor.", name="KnowledgeAgent")
    return Command(
        update={"messages": current_messages + [ai_message]},
        goto="__end__"
    )