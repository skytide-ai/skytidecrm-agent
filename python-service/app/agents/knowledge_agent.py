from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field, field_validator
from uuid import UUID
import pydantic_ai
from openai import AsyncOpenAI
from pydantic_ai import Agent, RunContext
from pydantic_ai.tools import ToolDefinition
import asyncio

# Importamos el estado global y funciones de Zep
from ..state import GlobalState
from ..zep import get_zep_memory_context, search_zep_facts, search_zep_sessions, search_zep_nodes
from ..db import supabase_client

# --- Cliente Pydantic AI ---
# Usamos un cliente de OpenAI independiente para este agente
# para mantener los contextos de herramientas separados.
client = AsyncOpenAI()

# --- Funciones Auxiliares para B√∫squeda Sem√°ntica ---

async def generate_embedding(text: str) -> List[float]:
    """
    Genera un embedding para el texto usando OpenAI.
    """
    try:
        response = await client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"‚ùå Error generando embedding: {e}")
        return []

async def search_knowledge_semantic(query: str, organization_id: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Busca informaci√≥n (servicios y archivos) usando b√∫squeda sem√°ntica en la knowledge_base.
    
    Args:
        query: Consulta de b√∫squeda del usuario
        organization_id: ID de la organizaci√≥n 
        limit: N√∫mero m√°ximo de resultados
    
    Returns:
        Lista de contenido encontrado (servicios y archivos) con sus metadatos
    """
    try:
        import time
        start_time = time.time()
        print(f"[{start_time:.2f}] --- Iniciando b√∫squeda sem√°ntica para: '{query}'")

        # Generar embedding de la consulta (esta funci√≥n s√≠ es async y debe ser esperada)
        query_embedding = await generate_embedding(query)
        
        if not query_embedding:
            return []
        
        embedding_time = time.time()
        print(f"[{embedding_time:.2f}] --- Embedding generado en {embedding_time - start_time:.2f}s")

        # Buscar en knowledge_base usando la sintaxis correcta para ejecutar
        # una llamada s√≠ncrona (rpc) desde un contexto as√≠ncrono.
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None, 
            lambda: supabase_client.rpc(
                'match_documents_by_org',
                {
                    'query_embedding': query_embedding,
                    'match_threshold': 0.2,
                    'match_count': limit,
                    'org_id': organization_id
                }
            ).execute()
        )

        rpc_time = time.time()
        print(f"[{rpc_time:.2f}] --- RPC a Supabase completado en {rpc_time - embedding_time:.2f}s")
        print(f"üîç DEBUG: RPC retorn√≥ {len(result.data) if result.data else 0} resultados")

        results_found = []
        seen_items = set()
        
        if result.data:
            for item in result.data:
                metadata = item.get('metadata', {})
                source_type = metadata.get('source_type')
                
                if source_type == 'service':
                    service_id = metadata.get('service_id')
                    # Evitar duplicados del mismo servicio
                    if service_id and service_id not in seen_items:
                        seen_items.add(service_id)
                        results_found.append({
                            'service_id': service_id,
                            'content': item.get('content', ''),
                            'similarity': item.get('similarity', 0),
                            'metadata': metadata
                        })
                        
                elif source_type == 'file':
                    # Para archivos, usamos una combinaci√≥n √∫nica de file_name + category
                    file_name = metadata.get('file_name', '')
                    category = metadata.get('category', '')
                    unique_key = f"{file_name}_{category}"
                    
                    if unique_key not in seen_items:
                        seen_items.add(unique_key)
                        results_found.append({
                            'content': item.get('content', ''),
                            'similarity': item.get('similarity', 0),
                            'metadata': metadata
                        })
        
        end_time = time.time()
        print(f"[{end_time:.2f}] --- B√∫squeda sem√°ntica completada en {end_time - start_time:.2f}s")
        return results_found
        
    except Exception as e:
        print(f"‚ùå Error en b√∫squeda sem√°ntica: {e}")
        return []

async def get_service_by_id(service_id: str) -> Optional[Dict[str, Any]]:
    """
    Obtiene informaci√≥n completa de un servicio por su ID.
    """
    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            lambda: supabase_client.table('services').select('*').eq('id', service_id).execute()
        )
        
        if result.data:
            return result.data[0]
        return None
        
    except Exception as e:
        print(f"‚ùå Error obteniendo servicio {service_id}: {e}")
        return None



# --- Modelos de Respuesta ---

class KnowledgeSearchResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de conocimiento con validaci√≥n estricta de tipos."""
    # Para servicios - UUID validado
    service_id: Optional[UUID] = Field(default=None, description="El ID √∫nico del servicio encontrado (debe ser UUID v√°lido).")
    service_name: Optional[str] = Field(default=None, min_length=1, description="Nombre del servicio encontrado.")
    requires_assessment: Optional[bool] = Field(default=None, description="Si el servicio requiere valoraci√≥n previa.")
    
    # Para informaci√≥n general (archivos)
    information_found: Optional[str] = Field(default=None, min_length=1, description="Informaci√≥n espec√≠fica encontrada (ubicaci√≥n, horarios, contacto, etc.)")
    source_type: Optional[str] = Field(default=None, description="Tipo de fuente: 'service' o 'file'")
    category: Optional[str] = Field(default=None, description="Categor√≠a del archivo si es de tipo 'file'")
    
    # Para casos especiales
    clarification_message: Optional[str] = Field(default=None, min_length=1, description="Mensaje para pedir aclaraci√≥n al usuario si la b√∫squeda es ambigua o no arroja resultados.")
    
    @field_validator('source_type')
    @classmethod
    def validate_source_type(cls, v):
        if v is not None and v not in ['service', 'file', 'multiple_services']:
            raise ValueError("source_type debe ser 'service', 'file' o 'multiple_services'")
        return v
    
    @field_validator('service_name', 'information_found', 'clarification_message')
    @classmethod
    def validate_non_empty_strings(cls, v):
        if v is not None and (not isinstance(v, str) or len(v.strip()) == 0):
            raise ValueError("El texto no puede estar vac√≠o")
        return v.strip() if isinstance(v, str) else v



class UserFactsResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de hechos del usuario."""
    facts_found: List[str] = Field(default_factory=list, description="Lista de hechos encontrados sobre el usuario.")
    summary: str = Field(description="Resumen de los hechos encontrados.")

class UserSessionsResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de conversaciones pasadas."""
    conversations_found: List[str] = Field(default_factory=list, description="Lista de fragmentos de conversaciones relevantes.")
    summary: str = Field(description="Resumen de las conversaciones encontradas.")

class UserInsightsResult(BaseModel):
    """Modelo para el resultado de la b√∫squeda de insights del usuario."""
    insights_found: List[str] = Field(default_factory=list, description="Lista de insights/res√∫menes encontrados.")
    summary: str = Field(description="Resumen de los insights encontrados.")

# --- Funci√≥n para control inteligente de herramientas ---
async def smart_tool_preparation(ctx: RunContext[GlobalState], tool_defs: List[ToolDefinition]) -> List[ToolDefinition]:
    """
    Funci√≥n prepare_tools que controla din√°micamente qu√© herramientas usar seg√∫n el contexto.
    Implementa an√°lisis sem√°ntico sin hardcodeo de palabras espec√≠ficas.
    """
    if not tool_defs:
        print("‚ö†Ô∏è WARNING: No hay herramientas disponibles para el agente")
        return []
    
    state = ctx.deps
    
    # Obtener el √∫ltimo mensaje del usuario
    last_message = state.get("messages", [])[-1] if state.get("messages") else None
    user_query = last_message.content if last_message else ""
    
    # üß† AN√ÅLISIS INTELIGENTE DEL CONTEXTO (SIN HARDCODEO)
    available_tools = []
    
    # 1. DETECTAR CAMBIO DE SERVICIO: An√°lisis contextual
    has_existing_service = state.get('service_id') is not None
    # CR√çTICO: Solo si hay indicadores EXPL√çCITOS de cambio, no solo que haya un servicio
    explicit_change_keywords = ["mejor", "cambio", "prefiero", "en lugar de", "no quiero", "diferente", "otro"]
    is_likely_service_change = (
        has_existing_service and 
        len(user_query.split()) >= 2 and  # M√°s de una palabra (sugiere especificidad)
        user_query.strip() != "" and  # No es mensaje vac√≠o
        any(keyword in user_query.lower() for keyword in explicit_change_keywords)  # Indicadores de cambio
    )
    
    # 2. DETECTAR NECESIDAD DE HISTORIAL: An√°lisis sem√°ntico
    # Si el usuario hace referencia a interacciones pasadas, usa pronombres demostrativos,
    # o palabras que sugieren historia personal
    query_suggests_history = (
        # Referencias temporales al pasado
        any(ref in user_query.lower() for ref in ["antes", "anterior", "previo", "ya", "otra vez"]) or
        # Referencias pronominales (el que, la que, ese, esta, etc.)
        any(ref in user_query.lower() for ref in ["el que", "la que", "ese", "esa", "esto", "eso", "aquel"]) or
        # Referencias a acciones previas del asistente
        any(ref in user_query.lower() for ref in ["dijiste", "mencionaste", "recomendaste", "sugeriste"]) or
        # Referencias a informaci√≥n personal (sin hardcodear condiciones m√©dicas espec√≠ficas)
        any(ref in user_query.lower() for ref in ["mi ", "mis ", "tengo ", "soy ", "problema"])
    )
    
    # 3. APLICAR L√ìGICA DE HERRAMIENTAS
    for tool_def in tool_defs:
        # SIEMPRE incluir knowledge_search - es la herramienta principal
        if tool_def.name == 'knowledge_search':
            available_tools.append(tool_def)
        
        # Herramientas de historial: solo si realmente se necesitan
        elif tool_def.name in ['search_user_facts', 'search_user_conversations', 'search_user_insights']:
            # REGLA: Solo incluir si hay evidencia sem√°ntica de necesidad de historial
            # Y NO es un cambio directo de servicio
            if query_suggests_history and not is_likely_service_change:
                available_tools.append(tool_def)
    
    # 4. LOGGING INTELIGENTE
    context_mode = "GEN√âRICO"
    if is_likely_service_change:
        context_mode = "CAMBIO_SERVICIO"
    elif query_suggests_history:
        context_mode = "NECESITA_HISTORIAL"
    
    print(f"üîß TOOLS PREPARADOS: {len(available_tools)} herramientas | Modo: {context_mode}")
    print(f"üß† Herramientas disponibles: {[t.name for t in available_tools]}")
    
    return available_tools

# --- Definici√≥n del Agente de Conocimiento ---
knowledge_agent = Agent[GlobalState, KnowledgeSearchResult](
    'openai:gpt-4o', 
    deps_type=GlobalState,
    output_type=KnowledgeSearchResult,  # ‚Üê FUERZA que el agente SIEMPRE devuelva KnowledgeSearchResult
    prepare_tools=smart_tool_preparation,
    system_prompt="""
    üß† ASISTENTE INTELIGENTE CON AN√ÅLISIS CONTEXTUAL

    Eres un asistente experto que entiende las intenciones del usuario y responde apropiadamente.
    
    **REGLA CR√çTICA: M√ÅXIMO UNA B√öSQUEDA POR CONSULTA**
    - Usa knowledge_search UNA SOLA VEZ por consulta del usuario
    - Si la primera b√∫squeda no encuentra informaci√≥n relevante, NO hagas m√°s b√∫squedas
    - Si no encuentras informaci√≥n espec√≠fica, ofrece conectar con un asesor
    
    **TU MISI√ìN:**
    - INFORMAR cuando el usuario solo quiere conocer sobre servicios
    - PREPARAR PARA AGENDAR cuando el usuario tiene intenci√≥n clara de reservar
    
    **AN√ÅLISIS DE INTENCIONES:**
    1. **CONSULTA INFORMATIVA**: Solo quiere saber ‚Üí Proporciona informaci√≥n, NO guardes service_id
    2. **INTENCI√ìN DE AGENDAR**: Quiere reservar ‚Üí Busca servicio Y guarda service_id para siguiente paso

    HERRAMIENTAS DISPONIBLES:
    
    1. B√öSQUEDA DE INFORMACI√ìN:
       - 'knowledge_search': Busca servicios Y informaci√≥n general usando IA sem√°ntica
       - NO necesitas palabras exactas: busca por significado e intenci√≥n
       - Encuentra autom√°ticamente: ubicaci√≥n, horarios, contacto, FAQ, pol√≠ticas, promociones, etc.
    
    2. B√öSQUEDA DE HISTORIAL DEL USUARIO (√∫salas cuando sea relevante):
       - 'search_user_facts': Busca hechos espec√≠ficos (servicios previos, preferencias, alergias, etc.)
       - 'search_user_conversations': Busca conversaciones pasadas (recomendaciones, quejas, etc.)
       - 'search_user_insights': Busca patrones de comportamiento del usuario
    
    FLUJO DE TRABAJO:
    
    üéØ CASO ESPECIAL - CAMBIO DE SERVICIO PARA AGENDAR:
    Si el usuario dice "quiero agendar [servicio]" o "mejor [servicio]" o "cambio a [servicio]":
    - SOLO haz 1 b√∫squeda: knowledge_search con el nombre del servicio
    - OBJETIVO: Encontrar el service_id √∫nicamente
    - NO busques horarios, disponibilidad, ni hagas m√∫ltiples consultas
    - Una vez que tengas el service_id, TERMINA
    
    FLUJO NORMAL:
    1. EVAL√öA LA CONSULTA: ¬øEl usuario menciona algo del pasado, historial, o "antes"?
       - SI: Usa herramientas de b√∫squeda de historial primero
       - NO: Procede con b√∫squeda de servicios
    
    2. B√öSQUEDA DE INFORMACI√ìN:
       - Usa 'knowledge_search' para encontrar servicios O informaci√≥n general
       - Para servicios: incluye nombre, descripci√≥n y detalles relevantes
       - Para info general: incluye contenido directo (ubicaci√≥n, horarios, etc.)
       - Si necesitas aclaraci√≥n, devuelve el mensaje y termina
    
    3. PERSONALIZACI√ìN CON HISTORIAL:
       - Si es relevante, busca informaci√≥n previa del usuario
       - Combina la informaci√≥n actual con el historial para dar respuestas personalizadas
    
    EJEMPLOS DE USO:
    
    INFORMACI√ìN GENERAL (b√∫squeda sem√°ntica inteligente):
    - "¬øD√≥nde quedan?" / "direcci√≥n" / "ubicaci√≥n" ‚Üí Encuentra info de ubicaci√≥n
    - "¬øA qu√© hora abren?" / "horarios" / "cu√°ndo atienden?" ‚Üí Encuentra horarios
    - "¬øTel√©fono?" / "contacto" / "WhatsApp" ‚Üí Encuentra info de contacto
    - "¬øOfertas?" / "descuentos" / "promociones" ‚Üí Encuentra promociones
    - "¬øDudas frecuentes?" / "FAQ" / "preguntas" ‚Üí Encuentra FAQ
    - "¬øPol√≠ticas?" / "reglas" / "t√©rminos" ‚Üí Encuentra pol√≠ticas
    
    SERVICIOS:
    - "Quiero un servicio que me ayude con [problema]" ‚Üí Busca servicios
    - "¬øCu√°nto cuesta [nombre del servicio]?" ‚Üí Busca servicio espec√≠fico
    
    HISTORIAL DEL USUARIO:
    - "¬øQu√© servicios me recomendaste antes?" ‚Üí search_user_conversations
    - "¬øCu√°les fueron mis servicios favoritos?" ‚Üí search_user_facts
    - "¬øTuve alg√∫n problema?" ‚Üí search_user_facts
    - "¬øHe venido antes por este tipo de problema?"
    
    üö® REGLAS CR√çTICAS:
    - NUNCA INVENTES informaci√≥n que no encuentres en las herramientas
    - NO menciones servicios espec√≠ficos a menos que la herramienta los devuelva expl√≠citamente
    - M√ÅXIMO UNA B√öSQUEDA: Si knowledge_search no encuentra informaci√≥n relevante, NO busques de nuevo
    - Si no encuentras informaci√≥n espec√≠fica, explica amablemente y ofrece conectar con asesor
    - SOLO usa datos reales retornados por las herramientas de b√∫squeda
    - NO hagas suposiciones sobre qu√© servicios podr√≠an existir
    - NO reformules la b√∫squeda con sin√≥nimos si la primera no fue exitosa
    
    üìù FORMATO DE RESPUESTAS:
    - Habla de manera natural y conversacional, evita listas t√©cnicas
    - Usa un tono amigable como "Te cuento que...", "Mira, tenemos...", "Perfecto, aqu√≠ est√°..."
    - Organiza la informaci√≥n de manera fluida, no como puntos de lista
    - Cuando presentes m√∫ltiples servicios, hazlo como si estuvieras recomendando personalmente
    - USA EMOJIS para hacer la conversaci√≥n m√°s natural y amigable üòä
    - NO uses asteriscos (**texto**) ni formato markdown para t√≠tulos
    - Escribe los nombres de servicios de forma natural, como en una conversaci√≥n normal
    
    üéØ OBJETIVO: Ayudar al usuario encontrando informaci√≥n relevante usando las herramientas necesarias.
    
    üß† S√â INTELIGENTE CON LAS HERRAMIENTAS:
    - EVAL√öA primero qu√© necesitas realmente buscar
    - NO uses todas las herramientas para cada consulta
    - Solo busca historial del usuario si es RELEVANTE para la consulta
    
    GU√çA DE CU√ÅNDO USAR CADA HERRAMIENTA:
    - 'knowledge_search': SIEMPRE para encontrar servicios, ubicaci√≥n, horarios, precios, info general
    - 'search_user_facts': SOLO si el usuario menciona "antes", "preferencias", o necesitas personalizar
    - 'search_user_conversations': SOLO si el usuario pregunta sobre conversaciones pasadas
    - 'search_user_insights': SOLO si necesitas patrones de comportamiento para recomendar
    
    ‚úÖ EJEMPLOS DE USO EFICIENTE:
    - "Quiero agendar una cita" ‚Üí SOLO knowledge_search (para encontrar servicio)
    - "¬øD√≥nde quedan?" ‚Üí SOLO knowledge_search (info general)
    - "¬øQu√© me recomendaste antes?" ‚Üí search_user_conversations + knowledge_search
    - "Mis servicios favoritos" ‚Üí search_user_facts + knowledge_search
    
    Siempre proporciona informaci√≥n √∫til y amigable en tu respuesta final.
    """
)

# --- Herramientas del Agente ---

@knowledge_agent.tool
async def knowledge_search(ctx: RunContext[GlobalState], query: str) -> KnowledgeSearchResult:
    """
    Busca cualquier informaci√≥n (servicios, ubicaci√≥n, horarios, contacto, etc.) usando b√∫squeda sem√°ntica.
    Si encuentra informaci√≥n relevante, la devuelve. Si no, ofrece escalaci√≥n a asesor.
    """
    state = ctx.deps
    organization_id = state.get("organization_id")
    
    if not organization_id:
        return KnowledgeSearchResult(clarification_message="Error: No se pudo identificar la organizaci√≥n.")
    
    print(f"üîç Buscando informaci√≥n para: '{query}' en organizaci√≥n {organization_id}")
    
    try:
        # Buscar con l√≠mite m√°s alto para capturar todos los chunks de un servicio
        matching_results = await search_knowledge_semantic(query, organization_id, limit=10)
        
        if not matching_results:
            return KnowledgeSearchResult(
                clarification_message=f"No encontr√© informaci√≥n espec√≠fica sobre '{query}'. ¬øTe gustar√≠a hablar con un asesor que pueda ayudarte mejor?"
            )
        
        # Analizar si es una b√∫squeda amplia de servicios
        query_lower = query.lower()
        is_broad_service_query = any(keyword in query_lower for keyword in [
            'servicios', 'qu√© tienen', 'qu√© ofrecen', 'cat√°logo', 'productos',
            'ofertas', 'que hacen', 'especialidades', 'opciones', 'disponible'
        ])
        
        # Analizar el tipo de resultado y consolidar informaci√≥n si es necesario
        best_result = matching_results[0]
        metadata = best_result.get('metadata', {})
        source_type = metadata.get('source_type')
        similarity = best_result.get('similarity', 0)
        
        print(f"‚úÖ Informaci√≥n encontrada: source_type={source_type}, similarity={similarity:.2f}")
        print(f"üîç B√∫squeda amplia de servicios detectada: {is_broad_service_query}")
        
        # üéØ UMBRAL DE SIMILARITY: Si la similarity es muy baja, no es informaci√≥n √∫til
        if similarity < 0.1:
            return KnowledgeSearchResult(
                clarification_message=f"No encontr√© informaci√≥n espec√≠fica sobre '{query}'. ¬øTe gustar√≠a que te conecte con un asesor para obtener informaci√≥n m√°s detallada? üòä"
            )
        
        # üß† AN√ÅLISIS SEM√ÅNTICO INTELIGENTE: ¬øLa informaci√≥n encontrada realmente responde a la pregunta?
        query_keywords = query.lower()
        content_preview = best_result.get('content', '').lower()
        
        # Para consultas espec√≠ficas, verificar que el contenido realmente contenga informaci√≥n relevante
        specific_queries = {
            'promocion': ['promocion', 'descuento', 'oferta', 'especial', '% off', 'rebaja'],
            'precio': ['precio', 'costo', 'valor', '$', 'pesos', 'tarifa'],
            'horario': ['horario', 'hora', 'atencion', 'abierto', 'cerrado', 'lunes', 'martes'],
            'ubicacion': ['ubicacion', 'direccion', 'donde', 'lugar', 'encontrar'],
            'contacto': ['telefono', 'celular', 'whatsapp', 'contacto', 'comunicar']
        }
        
        # Detectar si es una consulta espec√≠fica
        specific_query_detected = None
        for query_type, keywords in specific_queries.items():
            if any(keyword in query_keywords for keyword in keywords):
                specific_query_detected = query_type
                break
        
        # Si es una consulta espec√≠fica, verificar que el contenido la responda
        if specific_query_detected:
            relevant_keywords = specific_queries[specific_query_detected]
            content_is_relevant = any(keyword in content_preview for keyword in relevant_keywords)
            
            # Si el contenido NO es relevante para la consulta espec√≠fica
            if not content_is_relevant and similarity < 0.6:  # Umbral m√°s alto para consultas espec√≠ficas
                print(f"üö´ CONTENIDO NO RELEVANTE: Consulta sobre '{specific_query_detected}' pero contenido no contiene informaci√≥n relevante")
                return KnowledgeSearchResult(
                    clarification_message=f"No encontr√© informaci√≥n espec√≠fica sobre {query} en nuestra base de datos. ¬øTe gustar√≠a que te conecte con un asesor que pueda ayudarte con esta consulta? üòä"
                )
        
        # üìã MANEJO ESPECIAL PARA B√öSQUEDAS AMPLIAS DE SERVICIOS
        if is_broad_service_query and source_type == 'service':
            # Obtener servicios √∫nicos de los resultados
            unique_services = {}
            for result in matching_results:
                result_metadata = result.get('metadata', {})
                if result_metadata.get('source_type') == 'service':
                    service_id = result_metadata.get('service_id')
                    similarity_score = result.get('similarity', 0)
                    
                    # Solo incluir si tiene buena similarity
                    if similarity_score >= 0.1 and service_id:
                        if service_id not in unique_services or similarity_score > unique_services[service_id]['similarity']:
                            unique_services[service_id] = {
                                'content': result.get('content', ''),
                                'similarity': similarity_score,
                                'metadata': result_metadata
                            }
            
            # Si tenemos m√∫ltiples servicios, mostrarlos todos
            if len(unique_services) > 1:
                services_info = []
                for service_id, service_data in unique_services.items():
                    content = service_data['content']
                    # Extraer nombre del servicio de los metadatos o contenido
                    service_name = service_data['metadata'].get('service_name', 'Servicio')
                    services_info.append(f"üåü {service_name}\n{content}")
                
                consolidated_services = "\n\n".join(services_info)
                
                # Agregar sugerencia inteligente para m√°s detalles
                suggestion = "\n\nüí° Si quieres informaci√≥n m√°s detallada sobre alg√∫n servicio espec√≠fico (como precios, contraindicaciones, o cuidados), solo menciona el nombre del servicio que te interesa üòä"
                
                return KnowledgeSearchResult(
                    information_found=consolidated_services + suggestion,
                    source_type='multiple_services',
                    category='servicios'
                )
            
            # Si solo hay un servicio pero era una b√∫squeda amplia, sugerir m√°s opciones
            elif len(unique_services) == 1:
                # Procesar el √∫nico servicio encontrado pero agregar sugerencia
                service_data = list(unique_services.values())[0]
                content = service_data['content']
                suggestion = "\n\nüí° Este es uno de nuestros servicios. Si buscas algo espec√≠fico o quieres conocer otros servicios disponibles, puedes preguntarme por el tipo de servicio que te interesa üòä"
                
                return KnowledgeSearchResult(
                    information_found=content + suggestion,
                    source_type='service',
                    service_id=UUID(list(unique_services.keys())[0])
                )
        
        if source_type == 'file':
            # Es informaci√≥n general (ubicaci√≥n, horarios, etc.) - usar solo el mejor resultado
            content = best_result.get('content', '')
            print(f"üìÑ Content preview: {content[:100]}...")
            return KnowledgeSearchResult(
                information_found=content,
                source_type='file',
                category=metadata.get('category', 'general')
            )
        
        elif source_type == 'service':
            # Es informaci√≥n de un servicio - CONSOLIDAR TODOS LOS CHUNKS DEL MISMO SERVICIO
            service_id = metadata.get('service_id')
            service_name = metadata.get('service_name', 'Servicio')
            
            print(f"üîç DEBUG: Buscando chunks para service_id={service_id}")
            print(f"üîç DEBUG: Total de resultados recibidos: {len(matching_results)}")
            
            # DEBUG: Mostrar todos los resultados para diagnosticar
            for i, result in enumerate(matching_results):
                result_metadata = result.get('metadata', {})
                result_service_id = result_metadata.get('service_id')
                result_source_type = result_metadata.get('source_type')
                result_similarity = result.get('similarity', 0)
                print(f"üîç DEBUG: Resultado {i+1} - service_id={result_service_id}, source_type={result_source_type}, similarity={result_similarity:.2f}")
            
            # Filtrar todos los resultados del mismo servicio (con similarity m√≠nima)
            service_chunks = []
            for r in matching_results:
                r_metadata = r.get('metadata', {})
                r_service_id = r_metadata.get('service_id')
                r_source_type = r_metadata.get('source_type')
                r_similarity = r.get('similarity', 0)
                
                # Incluir si es del mismo servicio Y tiene buena similarity
                if (r_source_type == 'service' and 
                    r_service_id == service_id and 
                    r_similarity >= 0.1):
                    service_chunks.append(r)
            
            print(f"üìã DEBUG: Chunks del mismo servicio encontrados: {len(service_chunks)}")
            
            # Consolidar toda la informaci√≥n del servicio
            consolidated_content = []
            for chunk in service_chunks:
                chunk_content = chunk.get('content', '')
                chunk_similarity = chunk.get('similarity', 0)
                print(f"üìÑ Chunk encontrado (similitud: {chunk_similarity:.2f}): {chunk_content[:80]}...")
                consolidated_content.append(chunk_content)
            
            # Si no encontramos chunks adicionales, buscar otros chunks de servicio con similarity alta
            if len(service_chunks) == 1:
                print("üîç Solo se encontr√≥ 1 chunk, buscando otros chunks de servicios relacionados...")
                for r in matching_results:
                    r_metadata = r.get('metadata', {})
                    r_source_type = r_metadata.get('source_type')
                    r_similarity = r.get('similarity', 0)
                    
                    # Incluir otros chunks de servicios con alta similarity que no hayamos incluido ya
                    if (r_source_type == 'service' and 
                        r_similarity >= 0.3 and  # Similarity m√°s alta para otros servicios
                        r not in service_chunks):
                        
                        chunk_content = r.get('content', '')
                        print(f"üìÑ Chunk adicional relacionado (similitud: {r_similarity:.2f}): {chunk_content[:80]}...")
                        consolidated_content.append(chunk_content)
            
            # Unir todo el contenido con formato m√°s conversacional
            full_service_info = "\n\n".join(consolidated_content)
            print(f"üìã Informaci√≥n consolidada del servicio ({len(consolidated_content)} chunks)")
            
            # Formatear de manera m√°s conversacional
            conversational_info = f"Te cuento sobre {service_name} ‚ú®\n\n{full_service_info}"
            
            return KnowledgeSearchResult(
                information_found=conversational_info,
                source_type='service',
                service_id=UUID(service_id)
            )
        
        else:
            # Tipo de fuente desconocido, devolver contenido como informaci√≥n general
            content = best_result.get('content', '')
            return KnowledgeSearchResult(
                information_found=content,
                source_type='unknown'
            )
            
    except Exception as e:
        print(f"‚ùå Error en knowledge_search: {e}")
        return KnowledgeSearchResult(
            clarification_message="Hubo un problema al buscar informaci√≥n. ¬øTe gustar√≠a hablar con un asesor?"
        )


@knowledge_agent.tool
async def search_user_facts(ctx: RunContext[GlobalState], query: str) -> UserFactsResult:
    """
    Busca hechos espec√≠ficos sobre el usuario actual en su historial.
    √ötil para encontrar preferencias, servicios previos, problemas reportados, etc.
    
    Ejemplos de uso:
    - "servicios previos" 
    - "problemas reportados"
    - "preferencias de horarios"
    - "alergias o restricciones"
    """
    state = ctx.deps
    print(f"üîç Buscando hechos del usuario para: '{query}'")
    
    if not state.get("chat_identity_id"):
        return UserFactsResult(
            facts_found=[],
            summary="No hay informaci√≥n del usuario disponible en esta sesi√≥n."
        )
    
    # Construir user_id como en main.py
    if state.get("contact_id"):
        user_id = f"contact_{state['contact_id']}"
    else:
        user_id = f"chat_{state['chat_identity_id']}"
    
    try:
        facts = await search_zep_facts(user_id=user_id, query=query, limit=5)
        
        if facts:
            summary = f"Encontr√© {len(facts)} hechos relevantes sobre '{query}'"
            print(f"‚úÖ {summary}")
            return UserFactsResult(facts_found=facts, summary=summary)
        else:
            summary = f"No encontr√© informaci√≥n espec√≠fica sobre '{query}' en el historial del usuario."
            print(f"‚ùå {summary}")
            return UserFactsResult(facts_found=[], summary=summary)
            
    except Exception as e:
        print(f"‚ùå Error buscando hechos: {e}")
        return UserFactsResult(
            facts_found=[],
            summary="Error al buscar en el historial del usuario."
        )

@knowledge_agent.tool
async def search_user_conversations(ctx: RunContext[GlobalState], query: str) -> UserSessionsResult:
    """
    Busca conversaciones pasadas del usuario que contengan informaci√≥n espec√≠fica.
    √ötil para recordar discusiones previas, recomendaciones hechas, etc.
    
    Ejemplos de uso:
    - "citas canceladas"
    - "servicios recomendados"
    - "quejas o problemas"
    - "horarios preferidos"
    """
    state = ctx.deps
    print(f"üîç Buscando conversaciones del usuario para: '{query}'")
    
    if not state.get("chat_identity_id"):
        return UserSessionsResult(
            conversations_found=[],
            summary="No hay historial de conversaciones disponible en esta sesi√≥n."
        )
    
    # Construir user_id como en main.py
    if state.get("contact_id"):
        user_id = f"contact_{state['contact_id']}"
    else:
        user_id = f"chat_{state['chat_identity_id']}"
    
    try:
        conversations = await search_zep_sessions(user_id=user_id, query=query, limit=3)
        
        if conversations:
            summary = f"Encontr√© {len(conversations)} conversaciones relevantes sobre '{query}'"
            print(f"‚úÖ {summary}")
            return UserSessionsResult(conversations_found=conversations, summary=summary)
        else:
            summary = f"No encontr√© conversaciones previas sobre '{query}'."
            print(f"‚ùå {summary}")
            return UserSessionsResult(conversations_found=[], summary=summary)
            
    except Exception as e:
        print(f"‚ùå Error buscando conversaciones: {e}")
        return UserSessionsResult(
            conversations_found=[],
            summary="Error al buscar en el historial de conversaciones."
        )

@knowledge_agent.tool
async def search_user_insights(ctx: RunContext[GlobalState], query: str) -> UserInsightsResult:
    """
    Busca insights y res√∫menes sobre el comportamiento del usuario.
    √ötil para entender patrones, preferencias generales, perfil del cliente.
    
    Ejemplos de uso:
    - "perfil del cliente"
    - "comportamiento de compra"
    - "satisfacci√≥n con servicios"
    - "tendencias de uso"
    """
    state = ctx.deps
    print(f"üîç Buscando insights del usuario para: '{query}'")
    
    if not state.get("chat_identity_id"):
        return UserInsightsResult(
            insights_found=[],
            summary="No hay informaci√≥n de insights disponible en esta sesi√≥n."
        )
    
    # Construir user_id como en main.py
    if state.get("contact_id"):
        user_id = f"contact_{state['contact_id']}"
    else:
        user_id = f"chat_{state['chat_identity_id']}"
    
    try:
        insights = await search_zep_nodes(user_id=user_id, query=query, limit=3)
        
        if insights:
            summary = f"Encontr√© {len(insights)} insights relevantes sobre '{query}'"
            print(f"‚úÖ {summary}")
            return UserInsightsResult(insights_found=insights, summary=summary)
        else:
            summary = f"No encontr√© insights espec√≠ficos sobre '{query}'."
            print(f"‚ùå {summary}")
            return UserInsightsResult(insights_found=[], summary=summary)
            
    except Exception as e:
        print(f"‚ùå Error buscando insights: {e}")
        return UserInsightsResult(
            insights_found=[],
            summary="Error al buscar insights del usuario."
        )

# --- Funci√≥n de Entrada (Entrypoint) para el Grafo ---
from langgraph.types import Command
from langgraph.graph import END

async def run_knowledge_agent(state: GlobalState) -> Command:
    """
    Punto de entrada para ejecutar el agente de conocimiento.
    Ahora incluye contexto de memoria de Zep y usa Command pattern para evitar loops.
    """
    print("--- Ejecutando Knowledge Agent ---")
    
    user_query = state['messages'][-1].content
    
    # ‚úÖ CONSTRUIR HISTORIAL COMPLETO DE LA CONVERSACI√ìN
    history_messages = state["messages"][:-1]  # Todos excepto el √∫ltimo (mensaje actual)
    history_str = "\n".join([f"{msg.__class__.__name__}: {msg.content}" for msg in history_messages])
    
    # Detectar si venimos del AppointmentAgent
    is_service_resolution = "Necesito encontrar el servicio espec√≠fico que quiere agendar" in user_query
    print(f"üîç KNOWLEDGE AGENT: Modo resoluci√≥n de servicio: {is_service_resolution}")
    
    # Obtener contexto de memoria de Zep si hay chat_identity_id
    zep_context = ""
    if state.get("chat_identity_id"):
        thread_id = state['chat_identity_id']
        try:
            zep_context = await get_zep_memory_context(thread_id, min_rating=0.0)
        except Exception as e:
            print(f"‚ùå Error obteniendo contexto de Zep thread {thread_id}: {e}")
            zep_context = ""
    
    # Construir query enriquecido con contexto completo
    if is_service_resolution and zep_context:
        # Cuando venimos del appointment_agent, extraer el servicio del contexto
        # Usar un prompt MUY espec√≠fico que solo pida identificaci√≥n del servicio
        enhanced_query = f"IDENTIFICAR SERVICIO: Encuentra √∫nicamente el service_id del servicio mencionado en: {zep_context}. SOLO usa knowledge_search y retorna el resultado estructurado. NO generes texto conversacional."
        print(f"üîç KNOWLEDGE AGENT: Query de resoluci√≥n: {enhanced_query}")
    else:
        # üß† DETECCI√ìN INTELIGENTE DE CAMBIO DE SERVICIO (SIN HARDCODEO)
        
        # CONTEXTO: ¬øYa hay un servicio seleccionado?
        has_existing_service = state.get('service_id') is not None
        
        # AN√ÅLISIS SEM√ÅNTICO: ¬øEl mensaje sugiere un cambio REAL de servicio?
        # Solo debe activarse si realmente es un cambio, no la primera b√∫squeda
        is_service_change_request = (
            has_existing_service and  # Ya hay un servicio en contexto
            len(user_query.split()) >= 2 and  # Mensaje con suficiente contenido
            user_query.strip() != "" and  # No est√° vac√≠o
            # CR√çTICO: Solo si hay indicadores EXPL√çCITOS de cambio
            any(change_indicator in user_query.lower() for change_indicator in [
                "mejor", "prefiero", "cambio", "en lugar de", "no quiero", "diferente", "otro"
            ])
        )
        
        if is_service_change_request:
            # Prompt espec√≠fico para cambio de servicio - INTELIGENTE
            enhanced_query = f"""üéØ AN√ÅLISIS DE CAMBIO DE CONTEXTO

ESTADO ACTUAL: Ya hay un servicio seleccionado (ID: {state.get('service_id', 'N/A')})

HISTORIAL PREVIO:
{history_str}

NUEVA SOLICITUD: {user_query}

AN√ÅLISIS REQUERIDO:
- Analiza si el usuario quiere cambiar a un servicio diferente
- Si es as√≠, identifica el nuevo servicio mencionado en su solicitud
- Usa SOLO knowledge_search UNA VEZ para encontrar el service_id del nuevo servicio
- Tu √∫nica misi√≥n: encontrar el service_id del servicio solicitado
- NO busques informaci√≥n adicional como horarios o disponibilidad
- NO hagas m√∫ltiples b√∫squedas
- Una vez encontrado el service_id, tu tarea est√° completa

IMPORTANTE: Conf√≠a en tu an√°lisis sem√°ntico del contexto, no busques palabras espec√≠ficas."""
        else:
            # ‚úÖ FLUJO NORMAL CON HISTORIAL COMPLETO
            enhanced_query = f"""HISTORIAL DE LA CONVERSACI√ìN:
{history_str}

CONSULTA ACTUAL DEL USUARIO: {user_query}

IMPORTANTE: Si en el historial mencion√© servicios espec√≠ficos y el usuario se refiere a ellos (ej: "el primero", "ese", "el que dijiste"), identifica a qu√© servicio se refiere bas√°ndote en mis respuestas anteriores."""
        
        if zep_context:
            enhanced_query = f"Contexto del usuario (memoria): {zep_context}\n\n{enhanced_query}"
    
    # FORZAR EL USO DE HERRAMIENTAS - Nunca responder directamente
    # En Pydantic AI, forzamos el uso de herramientas via prepare_tools y validaci√≥n
    result = await knowledge_agent.run(enhanced_query, deps=state)
    
    # El resultado del agente es GARANTIZADO de ser KnowledgeSearchResult gracias a output_type
    tool_output = result.output
    print(f"‚úÖ STRUCTURED OUTPUT: {type(tool_output)}")
    print(f"üîç DEBUG KnowledgeSearchResult:")
    print(f"üîç service_id: {tool_output.service_id}")
    print(f"üîç service_name: {tool_output.service_name}")
    print(f"üîç information_found: {bool(tool_output.information_found)}")
    print(f"üîç clarification_message: {bool(tool_output.clarification_message)}")

    # Obtener mensajes actuales para conservar el historial
    current_messages = state.get("messages", [])

    if isinstance(tool_output, KnowledgeSearchResult):
        print(f"üîç DEBUG KnowledgeSearchResult:")
        print(f"üîç clarification_message: {bool(tool_output.clarification_message)}")
        print(f"üîç service_id: {tool_output.service_id}")
        print(f"üîç information_found: {bool(tool_output.information_found)}")
        
        if tool_output.clarification_message:
            # Si la herramienta devuelve un mensaje de clarificaci√≥n (incluso si fall√≥),
            # lo tratamos como la respuesta final de este turno para romper el bucle.
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=tool_output.clarification_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        elif tool_output.service_id:
            # üß† AN√ÅLISIS INTELIGENTE: ¬øEl usuario quiere agendar o solo informarse?
            user_wants_to_book = any(keyword in user_query.lower() for keyword in [
                "agendar", "reservar", "programar", "cita", "gustar√≠a agendar", "quiero agendar", "puedo agendar"
            ])
            
            if user_wants_to_book or is_service_resolution:
                # CASO 1: USUARIO QUIERE AGENDAR ‚Üí Guardar service_id en estado
                print(f"üéØ KNOWLEDGE AGENT: Usuario quiere agendar ‚Üí Guardando service_id: {tool_output.service_id}")
                
                result_data = {
                    "service_id": str(tool_output.service_id),
                    "messages": current_messages
                }
                if tool_output.requires_assessment is not None:
                    result_data["requires_assessment"] = tool_output.requires_assessment
                if tool_output.service_name:
                    result_data["service_name"] = tool_output.service_name
                    print(f"üìã KNOWLEDGE AGENT: Guardando service_name: {tool_output.service_name}")
                
                # Si estamos en modo resoluci√≥n, ir directo a AppointmentAgent
                if is_service_resolution:
                    print(f"üìã KNOWLEDGE AGENT: Modo resoluci√≥n ‚Üí AppointmentAgent")
                    return Command(update=result_data, goto="AppointmentAgent")
                else:
                    # Retornar al supervisor para continuar flujo de agendamiento
                    print(f"üìã KNOWLEDGE AGENT: Preparado para agendar ‚Üí Supervisor")
                    return Command(update=result_data, goto="Supervisor")
                    
            else:
                # CASO 2: SOLO CONSULTA INFORMATIVA ‚Üí NO guardar service_id, solo responder
                print(f"üí° KNOWLEDGE AGENT: Solo consulta informativa ‚Üí Respondiendo sin guardar estado")
                
                if tool_output.information_found:
                    from langchain_core.messages import AIMessage
                    ai_message = AIMessage(content=tool_output.information_found, name="KnowledgeAgent")
                    return Command(
                        update={"messages": current_messages + [ai_message]},
                        goto="__end__"
                    )
                else:
                    # Si por alguna raz√≥n no hay informaci√≥n, ir al supervisor
                    return Command(goto="Supervisor")
        elif tool_output.information_found:
            # Si se encontr√≥ informaci√≥n general (archivos o servicios), devolverla directamente y terminar
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=tool_output.information_found, name="KnowledgeAgent")
            
            # IMPORTANTE: Si es informaci√≥n de un servicio, tambi√©n actualizar el service_id en el estado
            update_data = {"messages": current_messages + [ai_message]}
            if tool_output.service_id:
                update_data["service_id"] = str(tool_output.service_id)  # Convertir UUID a string para el estado
                print(f"üìã KNOWLEDGE AGENT: Actualizando estado con service_id: {tool_output.service_id}")
                print(f"üìã KNOWLEDGE AGENT: update_data completo: {update_data}")
            if tool_output.service_name:
                update_data["service_name"] = tool_output.service_name
                print(f"üìã KNOWLEDGE AGENT: Actualizando estado con service_name: {tool_output.service_name}")
            if tool_output.requires_assessment is not None:
                update_data["requires_assessment"] = tool_output.requires_assessment
            
            return Command(
                update=update_data,
                goto="__end__"
            )
    
    elif isinstance(tool_output, UserFactsResult):
        if tool_output.facts_found:
            facts_text = "\n".join([f"- {fact}" for fact in tool_output.facts_found])
            response_message = f"{tool_output.summary}\n\n{facts_text}"
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=response_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        else:
            # Si no se encuentran hechos, regresar al supervisor para que decida
            return Command(goto="Supervisor")
    
    elif isinstance(tool_output, UserSessionsResult):
        if tool_output.conversations_found:
            conversations_text = "\n".join([f"- {conv}" for conv in tool_output.conversations_found])
            response_message = f"{tool_output.summary}\n\n{conversations_text}"
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=response_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        else:
            return Command(goto="Supervisor")

    elif isinstance(tool_output, UserInsightsResult):
        if tool_output.insights_found:
            insights_text = "\n".join([f"- {insight}" for insight in tool_output.insights_found])
            response_message = f"{tool_output.summary}\n\n{insights_text}"
            from langchain_core.messages import AIMessage
            ai_message = AIMessage(content=response_message, name="KnowledgeAgent")
            return Command(
                update={"messages": current_messages + [ai_message]},
                goto="__end__"
            )
        else:
            return Command(goto="Supervisor")

    # Si el resultado es un string directo (respuesta del LLM)
    elif isinstance(tool_output, str):
        from langchain_core.messages import AIMessage
        ai_message = AIMessage(content=tool_output, name="KnowledgeAgent")
        return Command(
            update={"messages": current_messages + [ai_message]},
            goto="__end__"
        )

    # Caso por defecto o si la salida no es lo que esperamos (ej. b√∫squeda vac√≠a)
    print("‚ö†Ô∏è El Knowledge Agent no produjo una salida clara. Terminando para evitar bucle.")
    from langchain_core.messages import AIMessage
    ai_message = AIMessage(content="No estoy seguro de c√≥mo proceder. ¬øPuedes reformular tu pregunta? Si lo prefieres, puedo contactar a un asesor.", name="KnowledgeAgent")
    return Command(
        update={"messages": current_messages + [ai_message]},
        goto="__end__"
    )