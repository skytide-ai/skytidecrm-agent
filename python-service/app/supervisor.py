from typing import Literal, Union
from openai import OpenAI
import pydantic_ai
from pydantic import BaseModel, Field
from pydantic_ai import Agent
from langgraph.types import Command
from langgraph.graph import END
from langchain_core.messages import AIMessage, HumanMessage, trim_messages

# Importamos el estado global y funciones de Zep
from .state import GlobalState
from .zep import get_zep_memory_context

# --- Cliente Pydantic AI ---
# Creamos un cliente de OpenAI para el supervisor
client = OpenAI()

# 1. Definimos los nombres de nuestros agentes
AGENT_NAMES = ("KnowledgeAgent", "AppointmentAgent", "EscalationAgent")
TERMINATE = "terminate"

# 2. Definimos el esquema de salida estructurada para el supervisor
class SupervisorOutput(BaseModel):
    next_agent: Literal[*AGENT_NAMES, TERMINATE] = Field(
        description="El nombre del siguiente agente que debe actuar o 'terminate' si la tarea ha concluido."
    )
    direct_response: str | None = Field(
        default=None,
        description="Respuesta directa del supervisor cuando puede manejar la consulta (para saludos, cortesÃ­a, etc.). Solo incluir si next_agent es 'terminate' y quieres responder directamente."
    )

# 3. Creamos la funciÃ³n del nodo supervisor que se usarÃ¡ en el grafo
async def supervisor_node(state: GlobalState) -> Command[Literal[*AGENT_NAMES, "__end__"]]:
    """
    Este nodo orquesta el flujo de trabajo. Analiza el mensaje mÃ¡s reciente del usuario
    y decide quÃ© hacer a continuaciÃ³n, utilizando el historial como contexto.
    """
    print("--- Supervisor ---")
    
    # DEBUG: Mostrar el estado que recibe el supervisor
    print(f"ğŸ” DEBUG Supervisor recibiendo estado:")
    print(f"ğŸ” service_id: {state.get('service_id')}")
    print(f"ğŸ” service_name: {state.get('service_name')}")
    print(f"ğŸ” organization_id: {state.get('organization_id')}")

    # --- 1. Extraer el Ãºltimo mensaje del usuario Y verificar si hay respuesta de agente ---
    latest_user_message = next((m for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), None)
    latest_ai_message = next((m for m in reversed(state["messages"]) if isinstance(m, AIMessage)), None)

    if not latest_user_message:
        print("âš ï¸ No se encontrÃ³ un mensaje de usuario para procesar. Terminando.")
        return Command(goto="__end__")
    
    # --- DETECTAR SI VENIMOS DE UN AGENTE QUE YA ACTUALIZÃ“ EL ESTADO ---
    # Si el Ãºltimo mensaje AI es mÃ¡s reciente que el Ãºltimo mensaje del usuario,
    # significa que un agente ya respondiÃ³ y debemos considerar el contexto actualizado
    if (latest_ai_message and 
        latest_user_message and 
        state["messages"].index(latest_ai_message) > state["messages"].index(latest_user_message)):
        
        print("ğŸ”„ DETECTADO: Regresando de un agente que ya procesÃ³ la solicitud")
        print(f"ğŸ”„ Ãšltimo mensaje AI: {latest_ai_message.name} - {latest_ai_message.content[:100]}...")
        
        # Si el KnowledgeAgent ya resolviÃ³ un servicio y hay service_id, ir directo a AppointmentAgent
        if (latest_ai_message.name == "KnowledgeAgent" and 
            state.get('service_id') and 
            any(keyword in latest_user_message.content.lower() for keyword in ["agendar", "reservar", "programar", "cita"])):
            
            print("ğŸ”„ KnowledgeAgent resolviÃ³ servicio + usuario quiere reservar â†’ AppointmentAgent")
            return Command(goto="AppointmentAgent")
        
        # Si cualquier agente ya terminÃ³ su tarea, terminar
        elif latest_ai_message.name in ["KnowledgeAgent", "AppointmentAgent", "EscalationAgent"]:
            print("ğŸ”„ Agente ya completÃ³ la tarea â†’ Terminando")
            return Command(goto="__end__")

    # --- 2. Preparar el contexto para el LLM ---
    history_messages = state["messages"][:-1]
    history_str = "\n".join([f"{msg.__class__.__name__}: {msg.content}" for msg in history_messages])
    
    zep_context = ""
    if state.get("chat_identity_id"):
        thread_id = state['chat_identity_id']
        try:
            zep_context = await get_zep_memory_context(thread_id)
            if zep_context:
                print(f"ğŸ§  Contexto Zep para thread {thread_id} encontrado.")
        except Exception as e:
            print(f"âš ï¸ No se pudo obtener contexto de Zep para thread {thread_id}. Error: {e}")

    # --- 3. Construir el Prompt para el Supervisor ---
    system_prompt = f"""Eres un asistente virtual experto de la empresa. Tu Ãºnica funciÃ³n es analizar el MENSAJE MÃS RECIENTE del usuario y decidir el siguiente paso.

    **Contexto de la ConversaciÃ³n (Mensajes Anteriores):**
    {history_str}

    **Memoria a Largo Plazo (Datos del Cliente):**
    {zep_context}

    **INSTRUCCIONES CRÃTICAS:**
    1.  Tu foco principal es el **"MENSAJE DEL USUARIO A PROCESAR"**.
    2.  Usa el contexto y la memoria SÃ“LO para entender la intenciÃ³n del mensaje actual.
    3.  **NO respondas a mensajes antiguos.** Tu tarea es actuar sobre el Ãºltimo input.
    4.  SÃ© decisivo y claro en tu enrutamiento.

    **Estado Actual:**
    - Service ID en estado: {state.get('service_id')}
    - Service Name en estado: {state.get('service_name')}

    **Reglas de Enrutamiento:**
    -   Para saludos, despedidas o charla casual: responde directamente y termina (`terminate`).
    -   Para consultas **VAGAS** (ej: "info", "ayuda"): responde pidiendo mÃ¡s detalles y termina (`terminate`).
    -   Para preguntas **ESPECÃFICAS** sobre servicios, precios, ubicaciÃ³n, horarios: enruta a `KnowledgeAgent`.
    -   Si un usuario quiere **reservar/agendar/programar** algo:
        -   Si NO hay service_id en estado O el usuario menciona un servicio especÃ­fico diferente al actual: enruta a `KnowledgeAgent` PRIMERO.
        -   Si YA hay service_id y el usuario solo quiere continuar con la reserva: enruta a `AppointmentAgent`.
    -   Si el usuario pide explÃ­citamente hablar con un **humano/asesor**: enruta a `EscalationAgent`.
    -   Si el mensaje del usuario es una simple confirmaciÃ³n (ej: "ok", "listo") y la tarea anterior ya se completÃ³: responde amablemente y termina (`terminate`).
    """
    
    user_input_for_llm = f"""
    **MENSAJE DEL USUARIO A PROCESAR:**
    "{latest_user_message.content}"
    """

    # --- 4. Invocar el LLM (Supervisor) ---
    supervisor_agent = Agent[GlobalState](
        'openai:gpt-4o',
        deps_type=GlobalState,
        result_type=SupervisorOutput,
        system_prompt=system_prompt
    )
    
    print(f"ğŸ” Invocando supervisor para el mensaje: '{latest_user_message.content}'")
    result = await supervisor_agent.run(user_input_for_llm, deps=state)
    next_agent_value = result.data.next_agent
    direct_response = result.data.direct_response
    
    print(f"âœ… Supervisor decidiÃ³ enrutar a: {next_agent_value}")

    # --- 5. Retornar el Comando ---
    if direct_response:
        print(f"ğŸ“ Supervisor generando respuesta directa: '{direct_response[:100]}...'")
        ai_message = AIMessage(content=direct_response, name="Supervisor")
        
        # Obtenemos los mensajes actuales para aÃ±adir el nuevo
        current_messages = state.get("messages", [])
        
        return Command(
            update={"messages": current_messages + [ai_message]},
            goto="__end__"
        )
    
    # Si no hay respuesta directa, simplemente enruta al siguiente agente.
    # El agente se encargarÃ¡ de aÃ±adir su propia respuesta al estado.
    # IMPORTANTE: Mantener el estado actual al enrutar
    
    print(f"ğŸ” DEBUG Supervisor enviando a {next_agent_value} con estado:")
    print(f"ğŸ” service_id: {state.get('service_id')}")
    print(f"ğŸ” service_name: {state.get('service_name')}")
    
    return Command(goto=next_agent_value)
